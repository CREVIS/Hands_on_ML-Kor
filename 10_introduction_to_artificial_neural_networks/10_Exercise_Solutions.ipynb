{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(그림 10-3처럼)원래의 인공 뉴런을 사용하여, `A ⊕ B`를 연산해주는 인공 신경망을 그려보자.\n",
    "\n",
    "   hint : A⊕B = (A⋀$\\neg$B)⋁($\\neg$A⋀B)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 분류기능을 하는데 로지스틱 회귀 분류모델을 퍼셉트론(예: 퍼셉트론 학습 알고리즘을 사용하여 학습한 단일 계층의 LTU)보다 더 선호하는 것인가? 어떻게 해야 퍼셉트론이 로지스틱 회귀 분류 모델이랑 성능이 비슷하도록 만들어 줄 수 있는가?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 MLP의 학습에서 왜 로지스틱 활성화 함수가 핵심이 되는 성분인가?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자주 사용이 되는 3가지의 활성화 함수의 이름들을 적어보아라. 그릴수도 있는가?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10개의 뉴런들로 이루어진 입력계층과 50개의 뉴런으로 이루어진 은닉계층, 3개의 뉴런을 가진 출력계층들로 만들어진 MLP가 있다고 하자. 또한 모든 인공 뉴런들은 ReLU활성화 함수를 사용한다.\n",
    "* 입력 행렬 **X**의 크기는 몇인가?\n",
    "* 은닉 계층에서 편향 벡터 **b**$_h$ 의 크기와 가중치 벡터 **W**$_h$의 크기는 몇인가?\n",
    "* 출력 계층의 편향 벡터 **b**$_0$ 와 가중치 벡터 **W**$_0$의 크기는 각각 얼마인가?\n",
    "* 신경망의 출력값 행렬 **Y**의 크기는 몇인가?\n",
    "* **X**, **b**$_h$, **W**$_h$, **b**$_0$, **W**$_0$에 대한 함수로 신경망 출력 행렬치 **Y**를 연산해주는 수식을 써보아라"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스팸메일과 그렇지 않은 메일을 구분할 때, 출력 계층에 얼마나 많은 뉴런들이 필요한가? 출력 계층에는 어떤 활성화 함수가 들어가 있어야 하는가? 만약 대신에 MNIST 데이터 세트를 사용한다면, 출력 계층에는 얼마나 많은 뉴런들이 있어야하는가? Chapter2에 있는 집값을 예측하는 신경망을 구축할 때면 출력 계층에는 얼마나 많은 뉴런들이 있어여하며 또 어떤 활성화 함수를 사용해야하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 알고리즘은 무엇이고 어떻게 동작하는가? 역전파 알고리즘과 역전모드 autodiff 사이의 차이점은 무엇인가??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP에 다루어야할 모든 하이퍼파라미터 갯수를 리스트로 만들수 있는가? 만약 MLP가 학습 데이터에 대해서 과잉학습을 한다면, 이를 해결하기 위해서 하이퍼 파라미터들을 어떻게 조절해야하는가??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "답답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MNIST 데이터 세트로 심층 MLP를 학습시키고 98%의 precision을 얻을 수 있는지 보자. Chapter9의 마지막 연습문제 처럼, 이것저것 멋대로 붙여보자. (예로, 체크포인트를 저장시켜준다던지, 중단시킬 경우 마지막 체크포인트를 불러온다던지, 요약을 추가해준다던지 텐서보드를 사용해서 그래프를 그려준다던지 등등)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 심층망을 만들어보자. 한줄이 추가된 것 빼면 이전에 했던것과 정확히 같다. 우리는 학습중에 정확도와 손실함수값을 측정하기 위해 `tf.summary.scalar()`를 추가하고, 우리가 텐서보드를 사용해 학습곡선을 쉽게 보기 좋게 해줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러 호출\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 텐서플로우 그래프를 돌리면서 노트북의 결과가 안정적으로 나올 수 있도록 설정\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST 특징 값들\n",
    "n_hidden1 = 300  #첫번째 은닉층의 뉴런수\n",
    "n_hidden2 = 100  #두번째 은닉층의 뉴런수\n",
    "n_outputs = 10  #출력 계층 뉴런의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph() # 그래프 초기화\n",
    "\n",
    "# 입력 뉴런 정의 (플레이스 홀더)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 정의\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 연산 정의\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률 설정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 학습에 사용할 Optimizer설정\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 높은 점수의 클래스를 출력하여 그 값들을 평균 내주어 정확도를 측정\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 변수 초기화 및 저장 함수 정의\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the directory to write the TensorBoard logs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재시간 출력 라이브러리 호출\n",
    "from datetime import datetime\n",
    "\n",
    "# 로그파일 생성 함수 정의 - 실행 시간을 이름으로 로그파일 생성\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"mnist_dnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텐서 보드 로그를 작성하는데 사용할 `FileWriter`를 만들어줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 초기에 멈추줄 수 있게 하는 것을 구현하지 않았을까? 이를 위해서는 검증 데이터 세트가 필요할 것이다.\n",
    "다행히, 텐서플로우의 `input_data()`함수가 주는 데이터 세트가 이미 학습 데이터(인스턴스 6만개, 데이터는 이미 섞여있음)와 검증 데이터세트(인스턴스 5000개), 테스트 데이터 세트(인스턴스 5000개)를 나누어 주었다. 그래서 우리는 쉽게 `X_valid`와 `y_valid`를 정의해줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우 MNIST 데이터 호출\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터 저장\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "# 학습 및 테스트 데이터 레이블 값 저장\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 검증 데이터 세트 데이터와 레이블 값 저장\n",
    "X_valid = mnist.validation.images\n",
    "y_valid = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 90.240% \tLoss: 0.35380\n",
      "Epoch: 5 \tValidation accuracy: 95.140% \tLoss: 0.17754\n",
      "Epoch: 10 \tValidation accuracy: 96.580% \tLoss: 0.12690\n",
      "Epoch: 15 \tValidation accuracy: 97.240% \tLoss: 0.10324\n",
      "Epoch: 20 \tValidation accuracy: 97.480% \tLoss: 0.09218\n",
      "Epoch: 25 \tValidation accuracy: 97.680% \tLoss: 0.08200\n",
      "Epoch: 30 \tValidation accuracy: 97.620% \tLoss: 0.07602\n",
      "Epoch: 35 \tValidation accuracy: 97.880% \tLoss: 0.07332\n",
      "Epoch: 40 \tValidation accuracy: 97.820% \tLoss: 0.07050\n",
      "Epoch: 45 \tValidation accuracy: 97.960% \tLoss: 0.06815\n",
      "Epoch: 50 \tValidation accuracy: 98.040% \tLoss: 0.06711\n",
      "Epoch: 55 \tValidation accuracy: 98.020% \tLoss: 0.06707\n",
      "Epoch: 60 \tValidation accuracy: 98.160% \tLoss: 0.06734\n",
      "Epoch: 65 \tValidation accuracy: 98.080% \tLoss: 0.06627\n",
      "Epoch: 70 \tValidation accuracy: 98.220% \tLoss: 0.06721\n",
      "Epoch: 75 \tValidation accuracy: 98.220% \tLoss: 0.06776\n",
      "Epoch: 80 \tValidation accuracy: 98.200% \tLoss: 0.06811\n",
      "Epoch: 85 \tValidation accuracy: 98.140% \tLoss: 0.06694\n",
      "Epoch: 90 \tValidation accuracy: 98.300% \tLoss: 0.06779\n",
      "Epoch: 95 \tValidation accuracy: 98.280% \tLoss: 0.06827\n",
      "Epoch: 100 \tValidation accuracy: 98.260% \tLoss: 0.06911\n",
      "Epoch: 105 \tValidation accuracy: 98.260% \tLoss: 0.06877\n",
      "Epoch: 110 \tValidation accuracy: 98.320% \tLoss: 0.06994\n",
      "Epoch: 115 \tValidation accuracy: 98.260% \tLoss: 0.07015\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001 # 학습 주기 횟수\n",
    "batch_size = 50 # 배치 사이즈\n",
    "# 전체 데이터 세트를 배치 사이즈로 나누어준 배치의 개수 \n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# 체크포인트 저장 경로\n",
    "checkpoint_path = \"/tmp/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_deep_mnist_model\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # 만약 체크포인트 파일이 존재한다면 모델을 재생성 하고 주기 횟수도 호출해온다.\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_deep_mnist_model\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 저장 및 테스트 데이터를 사용한 정확도 측정\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 출력\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
