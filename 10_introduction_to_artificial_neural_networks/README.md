=============**미완성**=============
======




Chapter 10. 인공신경망 입문
=======================
새가 우리에게 비행에 대한 영감을 주듯이, 자연은 수많은 발명품에 영감을 주었다. 이는 지능을 가진 기계를 만드는 방법에 대한 영감으로 뇌의 구조를 보는 것도 합리적인 것처럼 보인다. 이는 인공신경망(*Artificail Neural Networks*:ANNs)에 영감을 주는 아주 중요한 아이디어이다. 하지만 비록 비행기들은 새로부터 영감을 받긴했지만 그렇다고 날개를 휘젓지는 않는다. 비슷하게, 인공신경망도 생물학과는 꽤 달라지기 시작했다. 일부 연구원들은 심지어 우리가 생물학적 분석에서 벗어나야한다고 한다. ("뉴런"이라고 하기 보다는 유닛라고 말하는 것처럼) 생물학적으로 그럴듯한 시스템에 우리의 창의성이 제한 받지 않도록 하기 위해 말이다.

인공 신경망은 딥러닝의 핵심이 되는 것이다. 인공신경망은 다재다능하고 강력하며, 크기도 다양하다. 그리고 거대하고 매우 복잡한 기계학습 관련 일도 매우 이상적으로 수행해준다. 예를들면 수백만개의 이미지를 분류하는 것이나(예: Google Images), 음성인식 서비스를 더 강력하게 만들어주고(예: Apple의 Siri), 매일 수억명의 사용자에게 최적의 영상을 추천해주기도 한다(YouTube). 또한 바둑 게임에서 최고의 세계 챔피언에 대해서 이전의 수백만의 대국 기록들을 참고해서 자기 스스로와 대국을 해봄으로써 학습을 하기도 한다(Deep Mind의 AlphaGo).

이번 Chapter에서는 우리는 초창기 인공신경망에 대해서 빠르게 둘러보면서 인공신경망에 대해서 다루어 볼 것이다. 그리고나서 우리는 다중 레이어 퍼셉트론(*Multi-Layer Perceptrons*: MLPs)에 대해 볼 것이고 Chapter3에서 다루었던 MNIST 숫자 분류를 Tensorflow를 사용해 다중레이어 퍼셉트론을 구현해 볼 것이다.

# 생물학적 뉴런에서 인공뉴런까지
놀랍게도, 인공신경망은 오랬동안이나 우리 주위에 있었다. 1943년 신경생리학자 *Warren McCulloch*과 수학자 *Walter pitts*에 의해 인공 신경망이 처음 소개되었다. 이 ["A Logical Calculus of Ideas Immanent in Nervous Activity",](https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf) 논문을 보면 명제 논리를 사용해서 복잡한 연산을 수행하기 위해 어떻게 동물의 뇌에 있는 생물학적 뉴런이 함께 일을 하는지에 대해서 연산 모델로 간소화했다. 이것이 바로 첫번쩨 인공신경망의 구조이다. 후에 다룰 것이지만, 그이후로 수많은 구조들이 발명되어왔다.

1960년대까지 초기 인공신경의 성공은 우리가 곧 대화가 가능한 실제 지능을 가진 기계를 보게될 것이라는 믿음이 널리 퍼졌다. 하지만 이런 믿음이 오랫동안 충족되지 못한다는 것이 확실시 되면서 여기저기서 투자가 끊기고 인공신경망은 긴 암흑기에 접어든다. 1980대 초에 새로운 네트워크 구조 발명된 인공신경망에대한 관심이 부활했고, 좀 더 나은 학습 기술이 개발되었다. 하지만 1990년대 Chapter5에서 다루었던 Support Vector Machine 같은 인공신경망을 대체해주면서도 더 강력한 기계학습 기술이 대부분의 연구진들 사이에서 더욱 많이 사용되었다. 그당시에는 인공신경망보다 이론적 기반이나 학습 결과가 더 좋았기 때문이다. 그리고 마침내 우리가 그 인공신경망의 또다른 새로운 물결상에 있다. 이번 물결도 이전에 그랬던 것처럼 다시 죽게될 것인가? 이번 물결은 기존의 것과는 다르며 우리의 삶에 더욱 엄청난 영향력을 끼칠 것이라는 몇가지 이유가 있다.
* 이제는 신경망을 학습시키기에 충분할만큼 엄청난 양의 데이터가 있으며, 인공 신경망은 거대하고 복잡한 문제에 대해서 다른 기계학습 알고리즘보다 대체로 뛰어나다. 
* 1990년 이래로 컴퓨팅 파워의 엄청난 성장은 합당한 시간 동안에 거대한 신경망을 학습시키는 것을 가능하게 해주었다. 어느정도는 무어의 법칙 ((마이크로 칩의 저장 용량이 2년마다 배로 증가한다는 Intel 사의 G. Moore가 제창한 법칙)) 때문이지만 게임산업 덕분이기도 하다. 게입산업이 강력한 GPU가 나오는데 한 몫을 했다.
* 학습 알고리즘이 개선되어왔다. 1990년대에 사용하던 알고리즘과 한가지만 다르지만 이 조그마한 것이 커다란 긍정적인 효과를 일으켰다.
* 인공신경망에 기반이되는 이론들이 실제에서는 매우 좋아졌는데, 예를들어, 많은 사람들이 인공신경망 학습 알고리즘은 지역적인 최적점에 갇히는 경향때문에 망했다고 생각했지만 이제 더이상 실제로는 거의 발생하지 않는다.(혹은 보통 전역적인 최적점에 꽤 가까이 간다)
* 인공신경망은 돈과 진도의 선순환에 접어들었다. 인공신경망으로 만든 놀라운 제품들이 자주 기사 뉴스의 헤드라인으로 나오며, 이는 더 많은 주의를 끌 것이고, 결국 그런 상품들에 투자를 하게될 것이다. 계속해서 진보할 수록, 더 놀라운 삼품들이 나오게되고 투자가 이루어지면 그 돈으로 또 진보를 이루는 선순환에 접어든 것이다.

## 생물학적 뉴런
인공 뉴런에 대해서 얘기해보기전에, 아래 그림으로 나와있는 생물학적 뉴런을 빠르게 훑어보고 가자. (우리의 뇌같은)동물의 대뇌겉질에서 많이 발견되는 특이해보이는 세포이며, 이 세포는 세포핵을 포함하여 세포의 복합적인 구성요소로 세포체가 구성되어 있으며, 수상돌기(*dendrites*)라고 하는 것들이 나뭇가치 처럼 뻗어있고, 축색돌기(*axon*)라고 하는 매우 긴 가지를 가지고 있다. 축색돌기의 길이는 세포체보다 많게는 10배정도 길다. 축색돌기 끝에는 축색끝가지(*telodendria*)가 수많은 가지로 나뉘어져간다. 그리고 이런 가지들 끝에는 극도로 작은 신경접합부(*Synaptic terminals*, 짧게는 시냅스*Synapses*라고 한다)라는 것이 있다. 이는 다른 뉴런의 수상돌기에 연결되어있거나 신경세포체에 직접 연결되어있다. 생물학적 뉴런은 신호(*Signal*)이라고하는 짧은 전파를 다른 뉴런으로부터 신경접합부를 통해 받아들인다. 뉴런이 다른 뉴런으로부터 충분한 수의 신호를 받아들이면, 자기 자신만의 고유의 신호를 발생시킨다.
###### 그림 10-1. 생물학적 뉴런
![](../Book_images/10/10-1.png)

그러므로 단일 개체의 생물학적 뉴런은 간단한 방식으로 움직이는 것처럼 보이지만, 수백만의 뉴럽들의 거대한 망으로 조직화되어있으며, 각각의 뉴런들은 전형적으로 몇천개의 다른 뉴런들과 연결되어있다. 마치 개미총이 단순한 개미들이 협력으로 만들어지듯이 매우 복잡한 연산들도 간단한 뉴런들로 이루어진 거대한 네트워크를 통해서 수행되어진다. 이 생물학적 신경망(*Biological Neural Networks*: BNNs)의 구조는 여전히 활발한 연구 분야이지만, 뇌의 일부는 맵핑이 되어있어 뉴런들이 종종 아래의 보여지는 그림처럼 연이어 이어진 계층들로 보이기도 한다.

###### 그림 10-2. 생물학적 신경망에서의 다중 계층들 (인간피질)
![](../Book_images/10/10-2.png)
## 뉴런의 논리연산
Warrent McCulloch와 Walter Pitts는 생물학적 뉴런의 가장 간단한 모델을 제시했고, 이는 후에 하나 이상의 이진입력(on/off)를 받아 하나의 이진결과를 내주는 인공 뉴런이라고 알려진다. 인공뉴런은 특정 개수의 입력값이 활성화되면 결과값이 활성화되는 형식이다. McCulloch와 Pitts는 간소화된 모델로 우리가 원하는 어떠한 명제 논리를 연산해줄 수 있는 인공뉴런의 네트워크를 형성하는 것이 가능하다는 것을 보여주었다. 예를들어 뉴런이 최소 두개의 입력이 활성화되면 활성화된다고 가정하고 다양한 논리 연산을 수행해주는 인공신경망을 구현해보자. 아래의 그림을 참고하라
###### 그림 10-3. 간단한 논리 연산을 수행하는 인공신경망
![](../Book_images/10/10-3.png)

* 왼쪽에서 첫번째 네트워크는 간단한 항등함수이다. 만약 A뉴런이 활성화되면, A뉴런으로 부터 신호를 두개 받기때문에 C 뉴런또한 활성화될 것이다. 같은 원리로 A뉴런이 비활성화되면 C뉴런도 비활성화될 것이다.
* 두번째 네트워크는 AND 논리게이트를 수행해준다. C뉴런은 오직 A뉴런과 B뉴런이 활성화되어야만 활성화된다. (단일 신호만으로는 C뉴런을 활성화시키기에는 충분하지 않다.)
* 세번째 네트워크는 OR 논리게이트를 수행해준다. A뉴런이나 B뉴런 둘 중 하나나 혹은 둘 다 활성화되면 C뉴런이 활성화된다.
* 마지막은, 우리가 입력 연결이 뉴런의 활동을 억제할 수 있다고 가정하면 (생물학적 뉴런의 경우이기도 하다) 네번째 네트워크는 조금 더 복잡한 명제 논리를 연산한다. C뉴런은 A뉴런은 활성화되어있고 B뉴런이 비활성화되어 있어야만 활성화된다. A뉴런이 항상 활성화상태라고 하면 NOT 논리게이트도 얻게된다. C뉴런이 활성화될려면 B뉴런은 비활성화상태여야만 하기 때문이다.
## 퍼셉트론(Perceptron)
퍼셉트론(*Perceptron*)은 가장 간단한 인공신경망 구조중 하나로, 1957년 Frank Rosenblatt에 의해서 고안된 것이다. 이 아이디어는 선형 구분점 유닛(*Linear Threshold Unit*: LTU)라고 하는 조금은 다른 인공 뉴런에 기반을 한다. (아래 그림으로 나와있다) 입력과 출력이 (이진 입출력이였던 대신)모두 숫자값이며 각각의 입력 연결은 가중치와 연관이 있다. LTU는 입력값에 가중치를 곱한 값들을 모두 더하는 방식으로(z= w1⋅x1+w2⋅x2+...+wn⋅xn=w^T⋅x), 이 연산후에 계단함수(*Step Function*)를 적용한 값을 결과값으로 주게된다. h_w(x) = step(z) = step(w^T⋅x)
###### 그림 10-4. 선형 구분점 유닛 (Linear Threshold Unit)
![](../Book_images/10/10-4.png)

퍼셉트론에서 가장 자주 쓰이는 계단함수는 단위계단함수(*Heaviside step function*)이다. (아래에 그 공식이 나와있다) 때때로 신호함수가 대신 쓰이기도 한다.
###### Equation 10-1. 퍼셉트론에서 사용되는 일반적인 계단함수
![](../Book_images/10/Eq10-1.png)

단일 LTU는 간단한 선형 이진 분류에서 사용되어진다. 입력의 선형 조합을 연산하고 결과가 구분점을 넘어가면 Positive로, 아니면 Negative로 분류한다. (선형 회귀 분류나 선형 SVM처럼 말이다) 예를들어 iris 데이터셋을 (이전 Chapter에서 했던 것처럼 추가 편향치로 x0=1도 추가해서) 꽃잎의 길이와 너비 값으로 iris 꽃을 분류하는데 단일 LTU를 사용할 수 있다. LTU를 학습시킨다는 것은 w0,w1,w2가 올바른 값을 찾아가는 것을 의미한다.

퍼셉트론은 단일 계층의 LTU로 이루어져있으며, 각각의 뉴런들이 모든 입력과 연결되어있는 구조이다. 이러한 연결들은 입력 뉴런이라고 하는 특별한 뉴런을 지나오는 것으로 표현이 된다. 입력 뉴런들은 입력해준 값이 무엇이든 그 값을 그대로 전달해준다. 더 나아가 추가 편향치는 일반적으로 (x0=1)추가된다. 이 편향치는 전형적으로 편향 뉴런이라고 하는 특별한 종류의 뉴런으로 표현되며, 이는 1 값만 출력을 시켜준다.

두개의 입력을 받아 3개의 출력을 내는 퍼셉트론은 아래의 그림에 보여지고 있다. 이 퍼셉트론은 3개의 다른 이진 클래스로 동시에 인스턴스를 분류해줄 수 있으며, 이는 다중출력 분류 모델을 만든다.
###### 그림 10-5. 퍼셉트론 다이어그램
![](../Book_images/10/10-5.png)

그래서 퍼셉트론은 어떻게 학습이되는가? Frank Rosenblatt가 제안한 퍼셉트론 학습모델은 헵의 원칙(*Hebb's rule*)으로부터 크게 영감을 받았다. 1949년에 출간된 그의 책, 행동규칙(*"The Organization of Behavior"*)에서, Donald Hebb은 생물학적 뉴런이 종종 다른 뉴런을 작동시키면 그 두 뉴런 사이의 연결이 더욱 깅력헤진다고 주장했다. 이 아이디어는 후에 Siegrid Löwel이 다음과 같은 기억에 남을만한 구절로 정리해주었다. _**"Cells that fire together, wire together"(동시에 활성화되는 세포들, 서로가 연결된다)**_ 이러한 원칙은 후에 헵의 원칙(혹은 Hebbian Learning)이라고 알려지는데, 즉 두 뉴런 사이의 연결 가중치가 서로 같은 결과값을 가질때마다 그 가중치는 증가한다. 퍼셉트론은 네트워크에서 만들어지는 에러치를 account와 연관이 지어지는 규칙에 변형을 이용하여 학습이 이루어진다. 다시말하면 잘못된 결과를 내는 연결 가중치는 더이상 그 값이 강화되지 않는다는 것이다. 더 자세하게, 퍼셉트론은 한번에 하나의 학습 인스턴스을 입력하고, 각각의 인스턴스마다 예측치를 만들어낸다. 잘못된 결과를 내는 모든 출력 뉴런들에 대해서 올바른 예측을 해낸데 기여한 입력 가중치를 강화한다. 이 규칙은 아래 공식으로 보여지고 있다.
###### Euation 10-2 퍼셉트론 학습 규칙 (가중치 업데이트)
![](../Book_images/10/Eq10-2.png)

* **w_i,j**: i번째 입력 뉴런과 j번째 출력 뉴런 사이의 연결 가중치를 말한다.
* **x_i**: 현재 학습 인스턴스의 i번째 입력 값
* **ŷ_j**: 현재 학습 인스턴스의 j번째 출력 뉴런의 출력 값
* **y_i**: 현재 학습 인스턴스의 j번째 출력 노드의 목표 출력값
* **η**: 학습률

각각의 출력 노드의 의사 결정선은 선형인데, 그래서 퍼셉트론은 로지스틱 회귀 처럼 복잡한 패턴을 학습하는 능력은 없다. 
















**[뒤로 돌아가기](https://github.com/Hahnnz/Hands_on_ML-Kor/)**
