=============**미완성**=============
======


Chapter 11. 심층 신경망 학습시키기
=========

Chapter 10에서 인공 신경망에 대해서 알아보았고, 인공신경망을 직접 만들어 학습도 시켜보았다. 하지만 그 신경망은 은닉계층이 2개뿐인 매우 깊이가 얕은 구조였다. 고해상도 이미지에서 수백가지의 물건을 감지해주는 것과같이 아주 복잡한 문제를 해결해야한다면 어떨까? (말하자면) 아마 각 층마다 수백개의 뉴런을 포함하면서 수십만개의 연결선들을 가지는 10개의 계층 정도를 쌓은 좀 더 깊은 심층 신경망을 학습하여 사용하고 싶을 것이다. 이것은 간단히 산책하는 것마냥 쉬운일은 아니다.
* 먼저 심층신경망에 영향을 끼치고 매우 학습을 시키기가 어렵도록 하는 기울기가 사라지는 문제(*vanising gradient*) (혹은 관련된 또다른 문제인 그래디언트가 폭발적으로 증가하는 현상(*Exploding gradient*))을 해결해주어야 한다.
* 두번째로 이런 거대한 신경망에 대해서는 학습이 매우 느리다.
* 세번째, 수천만의 파라미터를 자기고 있는 모델을 학습 데이터 세트에 대해서 과잉학습을 할 위험이있다.

이번 Chapter에서는 우리는 이런 각각의 문제들을 직접 경험해볼 것이며, 문제들을 풀기 위한 기술들에 대하여 살펴볼 것이다. 우리는 먼저 기울기가 사라지거나 폭발적으로 증가하는 문제에 대해서 살펴보고 이 문제에 대한 가장 인기있는 해결방법을 살펴볼 것이다. 그 후에 일반적인 경사하강법과 비교하여 거대한 모델을 학습시키는 속도가 놀라울 수준으로 빠르게 해주는 몇가지 최적화 함수들도 살펴볼 것이다. 마지막으로 거대한 신경망에 대한 몇가지 괜찮은 정형화(regularization) 기술들도 살펴볼 것이다.

# 기울기가 사라지거나 폭발적으로 증가하는 문제
Chapter 10에서 다루었듯이, 역전파(backpropagation) 알고리즘은 출력 계층에서 입력계층으로 내려가면서 에러 미분치를 전파하는 식으로 작동한다. 한번 알고리즘이 신경명에 있는 각각의 파라미터들에 대해서 손실 함수치에 대한 기울기를 계산하면, 경사 하강법 단계로 각각의 파라미터를 업데이트 하는데 이러한 기울기 값들을 사용한다. 

불행하게도, 기울기는 종종 알고리즘이 하위 계층으로 내려가면서 그 값이 점점 작아진다는 것이다. 결국에는 경사 하강법은 하위에 있는 연결 가중치들은 실제로 바뀌지 않게되며, 학습은 절대로 좋은 솔루션에 도달할 수 없게된다. 이를 기울기가 사라지는 문제(*Vanishing gradient*)라고한다. 일부 경우에서는, 반대인 경우도 있는데, 기울기가 점점 커지게 되고 그래서 많은 계층들이 유별나게 커다란 가중치를 업데이트하게 되고, 알고리즘은 발산하게된다. 이를 기울기가 폭발적으로 증가는 문제(*Exploding gradient*)문제라고 하며, 이는 회귀 신경망(*Recurrent Neural Network*)에서 자주 목격되는 문제이기도 하다.(Chapter 14 참고) 좀 더 일반적으로, 심층 신경망을 불안정한 기울기값들에 대해서 고통을 받게돠고, 각각의 계층들이 서로 다른 속도로 학습을 하게 될 것이다.

비록 불핸한 행동들이 꽤 오랫동안 경험에 기반하여 관측되어왔지만,(이런 문제들 때문에 심층 신경망이 오랫동안 버려져있던 이유중 하나이다) 2010년에 와서 이런 문제에 대한 이해를 할 수 있게 되었다. Xavier Glorot과 Yoshua Benigo가 작성한 "Understanding the Difficulty of Training Deep Forward Networks"라는 제목의 [논문](https://goo.gl/1rhAef)에서는 그 당시에 가장 인기 있던 가중치 초기화 기법인 평균 0과 표준분산 1로하는 정규분포를 사용한 이름 그대로의 랜덤 초기화와 로지스틱 시그모이드 활성화 함수의 조합을 포함하여 몇가지 의심스러운 점을 발견했다. 짧게 말하면, 그들은 위의 초기화 기법과 활성화 함수로 각각의 계층들의 출력에 대한 변화량이 입력 값의 변화량보다 더 크다는 것을 보여주었다. 신경망을 실행시켜 쭉 진행시켜보면, 활성화 함수들이 최상위 계층에서 포화 상태가 될 때까지 각각의 계층의 변화향이 계속해서 증가하게된다. 이는 실제로 로지스틱 함수가 평균값으로 0이 아닌 0.5를 가지는 점으로 인해 상황이 더 나빠지게 한다. (하이퍼블릭 탄젠트 함수는 평균값으로 0을 가지는데 이는 심층 신경망에서 로지스틱 함수보다는 좀 더 나은 결과를 보여준다)

아래 그림에 나와있는 로지스틱 활성화 함수를 보면, 입력이 양의 방향이든 음의 방향이든 커질수록, 함수는 0과 1로 포화되게 되며, 기울기는 갈수록 0에 수렴되게 된다. 그러므로 역전파를 하게된다면, 신경망을 역으로 전파할 기울기가 없고, 아주 약간의 기울기가 존재하던 것들 마저도 역전파가 최상위 계층에서 내려오면서 전파 하면서 점점 희석되어 결국에는 없어기제게되어 최종적으로는 남는 그래디언트는 없어지게된다. 
###### 그림 11-1. 로지스틱 활성화 함수 포화 상태
![](../Book_images/11/11-1.png)

## Xavier와 He 초기화 기법
그들의 논문에서, Glorot과 Benigo는 이런 문제들을 상당히 환화시켜줄 방법을 제시했다. 예측을 내릴때는 순방향으로, 그래디언트를 역전파 시킬때에는 역방향으로 양방향 모두 적절히 신호가 흘러야만한다. 신호가 죽어서도 폭발적으로 증가해서 포화 상태가 되는 등 그 어떠한 것도 원하지 않는다. 신호가 적절히 흐르게하기 위해서, 이 논문의 역자들을 각각의 계층에서 계층의 입력의 변화량과 돌일한 출력 변화량을 가지고, 역방향에서 각각의 계층을 통과하기 전과 후가 같은 변화향을 가지는 기울기 값들이 필요했다. (수학적인 것에 좀 더 관심이 있다면 논문을 참고하자) 계층의 입력과 출력의 연결수가 같을지라도 보장을 할 수는 없지만, 실제 세계에서는 잘 작동한다고 증명된 좋은 절충점을 제시했다. 연결 가중치는 아래의 공식 11-1에서 묘사된대로 랜덤하게 초기화되어야한다는 것이며, n_input과 n_output은 초기화시킬 가중치를 가지고 있는 계층의 입력과 출력 연결의 수를 뜻한다. (*fan-in*, *fan-out*이라고도 한다) 이런 초기화 전략을 저자의 이름을 따서 Xavier(혹은 Glorot) 초기화 기법이라고 부르게된다.
###### 공식 11-1. Xavier 초기화 기법(로지스틱 활성화 함수를 사용할 때)
![](../Book_images/11/Eq11-1.png)

입력 연결의 수가 출력 연결의 수와 거의 같으면, 더 간단한 식을 얻는다. ()






비록ㅎ

**[뒤로 돌아가기](../index.md)**

**[위로 올라가기]()**
