Chapter 6. 의사결정트리
=========
SVM처럼, *의사결정나무(Decision Tree)* 도 분류와 회귀 모두 수행해주고 심지어 다중결과출력도 해주는 다재다능한 기계학습 알고리즘이다. 매우 강력한 알고리즘이며 복잡한 데이터 세트에 대해서도 학습을 잘 수행한다. 예를들어 Chapter 2에서 캘리포니아 하우징 데이터세트로 `DecisionTreeRegressor`모델을 학습시켜보았고 완벽하게 학습을 진행해주었다(실제론 과잉학습이였지만).

의사결정나무는 또한 랜덤포레스트(Random Forest)의 기반이되는 요소이며. (다음 Chapter에서 다룬다) 이는 오늘날 가장 강력한 기계학습 알고리즘중에 하나이다.

이번 Chpater에서는 의사결정나무로 어떻게 학습시키고 시각화하여 예측값을 내리는지에 대해 이야기해볼것이다. 그리고 우리는 Scikit-Learn으로 CART 학습 알고리즘을 사용해볼것이고, 의사결정나무를 어떻게 학습을 정형화하고 회귀를 하는지에 대해서도 이야기해볼 것이다. 끝에서는 의사결정 나무의 한계점에 대해서도 다루어 볼 것이다. 

# 의사결정나무 학습 및 시각화
의사결정나무를 이해하기 위해서, 일단 한번 만들어보고 어떻게 예측을 내리는지 살펴보자. 다음 코드는 iris 데이터 세트에 대해서 `DecisionTreeClassifier`를 학습시키는 코드이다.
```
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:, 2:] # petal length and width
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)
```
*iris_tree.dot*이라고 하는 그래프 정의 파일을 만들어주기 위해 `export_graphviz()`함수를 사용해서 학습된 의사결정트리를 시각화해보자. 
```
from sklearn.tree import export_graphviz

export_graphviz(
        tree_clf,
        out_file=image_path("iris_tree.dot"),
        feature_names=iris.feature_names[2:],
        class_names=iris.target_names,
        rounded=True,
        filled=True
    )
```
*graphviz* 패키지에서 제공하는 **dot**이라는 명령어를 사용해서 *.dot*이라는 확장자를 *pdf*나 *png*같은 다양한 확장자로 변환해줄 수 있다. 아래의 명령어는 *.dot*이라는 파일을 *.png* 이미지 파일로 변환해주는 것이다.
```
$ dot -Tpng iris_tree.dot -o iris_tree.png
```
아래의 그림은 우리의 첫번째 의사결정트리를 보여준다.
###### 그림 6-1. Iris 데이터세트 의사결정트리
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-1.png)

# 예측하기
이제 그림 6-1에 나와있는 의사결정트리가 어떻게 예측을 하는지 살펴보자. iris 꽃을 찾아서 이를 분류하고 싶다고 해보자. 먼저 root node (깊이:0,최상단)에서 시작한다. 이 노드는 주어진 꽃의 꽃잎의 길이가 2.45cm가 넘는지 안넘는지 물어볼 것이다. 2.45cm보다 작다고 하면 root노드의 왼쪽 자식 노드 (깊이:1, 왼쪽)로 내려갈 것이다. 이러한 경우, 이를 리프노드(leaf node: 자식 노드를 가지고 있지 않은 노드)라고 하며, 이제 더이상의 질문은 하지 않는다. 이제 결과로 의사결정트리가 우리가 준 꽃은 Iris-Setosa라고 하는 꽃으로 (class=Setosa) 예측해줄 것이다.

이제 우리가 또 다른 꽃을 분류해볼것인데 이번에는 꽃잎의 길이가 2.45가 넘는다고 해보자. 그렇다면 이제 root노드의 오른쪽 자식 노드 (깊이:1, 오른쪽)로 내려갈 것인데, 이 노드는 리프노드가 아니다. 그래서 이제 또 다른 질문을 할 것이다. 이번엔 꽃잎의 너비가 1.75가 되는지 물어볼 것이다. 만약 1.75cm보다 작다면 그 꽃은 Iris-Vercicolor (깊이:2,왼쪽)으로, 크다면 Iris-Verginica (깊이:2, 오른쪽)이라고 분류할 것이다. 
```
의사결정나무의 장점중 하나는 요구하는 데이터가 적다는 것이다. 게다가 스케일링을 할 필요까지도 없다.
```
노드의 `Samples` 속성은 얼마나 많은 학습 인스턴트들이 적용되었는지를 센 것이다. 예를들어, 100개의 학습 인스턴스가 2.45cm보다 긴 꽃잎 길이(깊이:1,오른쪽)를 가지고 있으며 그 중 54개는 꽃잎의 넓이가 1.75cm이하인 것(깊이:2, 왼쪽)이라고 할 수 있다. 노드의 `value`라는 속성은 얼마나 많은 학습 인스턴스의 수가 각각의 클래스에 적용되었는지를 센 것으로, 예로 맨 아래의 오른쪽 노드는 Iris-Setosa 0개, Iris-Versicolor 1개, Iris-Virginice 45개가 적용이 되어있다. 마지막으로, 노드의 `gini`속성은 *불순도(impurity)* 을 측정한 것이다. 모든 학습 인스턴스가 올바른 클래스로 분류되어 있다면 그 노드를 *순수"(pure)"* 하다고 한다. `gini=0` . 예를들어서 `깊이:1,왼쪽`노드는 Iris-Setosa 클래스에만 학습 인스턴스가 있기 때문에, 이 노드는 "순수"하며, gini 수치는 0이다. 아래의 공식은 학습 알고리즘이 어떻게 i번째 노드에 대해서 gini 수치값 G1를 연산하는지 보여준다. 예를들면, `깊이2,왼쪽`노드는 1-(0/54)^2 - (49/54)^2 - (5/54)^2 ≈ 0.168로 gini 노드 값을 가지고 있다. 또 다른 불순도 측정법은 곧 얘기할 것이다.
###### Equation 6-1. Gini 불순물식
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/Eq6-1.png)

* p_i,k는 i번째 노드에서 학습 인스턴스 사이의 클래스 k 인스턴스에 대한 비율을 나타낸다.
```
Scikit-Learn은 CART 알고리즘을 사용하는데, 이는 오직 이진 트리를 만들어낸다. 리프노드가 
아닌 노드들은 무조건 2개의 자식 노드를 가진다. (질문도 Yes/No로 답을 받기때문이다) 하지만
ID3같은 다른 알고리즘들은 2개 이상의 자식노드를 가지는 노드들로 의사결정트리를 만들어내기도 한다.
```
아래의 그림은 의사결정트리의 의사결정선을 보여준다. 이 기법의 수직선은 `루트노드(깊이:0):꽃잎길이=2.45cm`에대한 의사결정선을 나타낸다. 왼쪽의 구역은 완전 순도가 높기때문에(Iris-Setosa만 있음), 더이상 나누어질 수가 없다. 하지만 오른쪽 구역은 불순도가 높다. 그래서 깊이-1의 오른쪽 노드는 (대시선으로 나타냄) 꽃잎 넓이 1.75cm 지점에서 나누어진다. 최대깊이가 2이기 때문에, 의사 결정 트리는 여기서 멈춘다. 하지만 만약 최대 깊이가 3이였다면, 깊이-2에 있는 노드들도 추가적으로 의사결정선이 발생했을수도 있다. (점선으로 표시되어있음)

###### 그림 6-2. 의사결정나무의 의사결정선
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-2.png)
```
학습 모델 해석 : 하양상자 VS 까망상자

보다시피 의사결정나무는 꽤 직관적이고 그 결정에 대해서 설명하기가 쉽다. 대조적으로 ,곧 볼 것이지만,
랜덤포레스트나 신경망 같은 경우는 일반적으로 까망상자모델로 불린다. 신경망이나 랜덤포레스트도 예측을
아주 잘하지만 그 결과에대해서 왜 이렇게 예측을 잘 내릴 수 있었는지 설명하기가 매우 어렵다. 예를들어
신경망이 사진에 있는 특정 사람이 있다고 말하면 어떤 속성이 그 예측결과에 영향을 주었는지 알아내기가
어렵다. 그 모델이 그 사람의 눈, 입, 코, 신발등을 알아차린 것일까? 아니면 그 사람이 앉아있는 소파가
영향을 끼친 것일까? 그에비해, 의사결정나무는 간단하고 좋은 분류 규칙을 가지고 있다.
```
# 클래스 확률치 평가
의사결정나무는 인스턴스 특정 클래스 k에 속해있는 확률 또한 평가할 수 있다. 먼저 해당 인스턴스에 대해서 리프 노드를 찾아가는 나무를 실행해보고 그 노드에 있는 클래스 k에 대한 학습 인스턴스의 비율를 받는다. 예를들어 꽃잎의 길이가 5cm, 넓이가 1.5cm라면 이에 상응하는 리프노드는 깊이-2의 왼쪽노드이며, 그래서 의사결정나무는 Iris-Setosa에 대해서는 0%(0/54), Iris-Versicolor에 대해서는 90.7%(49/54), Iris-Virginice에 대해서는 9.3%(5/54)라고 결과를 보여줄 것이다. 클래스에 대해서 예측해달라고 한다면 Iris-Versicolor(Class 1)이라고 결과를 보여줄 것이다. 한번 직접 확인해보자.
```
>>> tree_clf.predict_proba([[5, 1.5]])
array([[ 0.        ,  0.90740741,  0.09259259]])
>>> tree_clf.predict([[5, 1.5]])
array([1])
```
완벽하다! 평가된 확률치들은 그림 6-2 (예로 꽃잎의 길이가 6cm길이고 넓이가 1.5cm라면)하단 오른쪽 직사각형들로 표현되어있다.  

# CART 학습 알고리즘
Scikit-Learn은 분류 및 회귀 트리(**C**lassification **A**nd **R**egression **T**ree)라고 하는 것을 의사결정나무를 학습 시키는 데에 이용한다. (이를 또한 "자라나는" 나무라고도 한다.) 아이디어는 꽤나 간단하다. 알고리즘은 먼저 단일 특징 K와 경계값(예:"꽃잎의 길이<2.45cm") tk를 이용해서 학습데이터를 두개의 서브셋으로 나눈다. 어떻게 하면 k와 tk를 고를 수 있을까? 가장 순도가 높은 서브셋(이 서브셋 크기로 가중치를 매긴)을 만드는 (k,tk)쌍을 찾는 것이다. 이 알고리즘이 최소화하고자 하는 손실함수는 아래의 공식으로 나타내었다.
###### Equation 6-2. 분류에서의 CART 손실함수
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/Eq6-2.png)

학습 데이터 세트가 두개로 성공적으로 나뉘고 나면, 같은 논리값들을 사용하여 서브셋을 쪼갠다. 그리고 그 서브셋의 서브셋 그렇게 반복적으로 나아간다. 그렇게 쪼개다가 최대깊이(`Max_depth`라는 하이퍼파라미터로 정의됨)까지 도달하게 되거나 혹은 불순도를 줄일 수 없을때 쪼개기 과정을 멈추게 된다. 몇가지 (곧 설명할) 다른 하이퍼파라미터들은 추가적 중단 조건을 컨트롤한다. (`Mmin_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, max_leaf_nodes`)
```
보다시피, CART 알고리즘은 아주 탐욕적인 알고리즘인데, 상위 레벨에서 최적분할치를 욕심을 내면서 
찾으며, 각각의 레벨마다 이 프로세스를 반복한다. 그 분할치가 최고로 낮은 불순도를 확인하지는
않는다는 것이다. 탐욕적인 알고리즘은 종종 타당한 좋은 솔루션을 찾기도 하지만, 그것이 최적의
솔루션이라는 보장이 없다.
```
불행하게도 최적의 트리를 찾는 것은 NP-완전문제가 되는 것으로 알려져있다. 이는 심지어 꽤 작은 학습 데이터 세트에 대해서도 문제를 아주 다루기 힘들게 만들면서도 O(exp(m))시간을 요구한다. 이는 왜 우리가 적당히 좋은 솔루션에 만족해야만 하는 이유이다.

# 연산 복잡도
예측을 한다는 것은 루트 노드에서 리프 노트까지 의사결정트리를 거쳐가야만한다. 의사결정트리는 일반적으로 거의 균형을 유지하기때문에, 의사결정트리를 거쳐가는 것은  거의 O(log_2(m))개의 노드수를 거쳐가야만 한다. 각각의 노드는 하나의 특징에 대한 값만 확인을 하기 때문에 전체 예측 복잡도는 특징의 수와는 관계없이 O(log_2(m))이다. 그래서 거대한 학습 데이터 세트에 대해서도 예측프로세스가 매우 빠르다.

하지만 학습 알고리즘은 각각의 노드에 있는 모든 샘플 수에 대해서 모든 특징들(`max_features`가 설정되어있다면 그보다 적게)과 비교를 하게된다. 이는 O(n X m log(m))만큼의 학습 복잡도를 가지게 된다. (몇천개 인스턴스 이하를 가진)크기가 작은 데이터 세트에 대해서는 Scikit-Learn이 데이터 사전정렬(`presort=True`라고 설정)하여 학습 속도를 올릴 수 있지만, 이는 커다란 데이터 세트에 대해서는 학습속도가 상당히 느려지게된다는 점이 있다. 
# Gini 불순도인가 아니면 엔트로피인가?
기본값으로 mini 불순도 측정법이 사용된다. 하지만 `criterion` 하이퍼파리미터를 *“entropy”* 라고 설정하면,  엔트로피 불순도 측정법을 사용할 수 있다. 엔트로피의 개념은 열역학에서 분자구조의 변형치(에너지)을 측정하는 것으로 부터 유래한 것이다. 엔트로피는 분자 배열이 잘 되어 있으면 0에 가깝다. 이는 어떤 메세지의 평균 정보 내용을 측정하는 새넌의 정보이론(Shannon’s Information Theory)을 포함하여 후에 다양한 분야로 퍼지게 되었으며, 새넌의 정보이론에서 엔트로피는 모든 메세지의 내용이 동일하면 0이라는 것이다. 기계학습에서도 불순도 측정에 자주 사용하고는 하는데 오직 한 클래스만의 인스턴스를 가지고 있을때 엔트로피가 0이라고한다. 아래의 공식은 i번째 노드에 대한 엔트로피 정의를 보여준다. 예를들어 그림 6-1에 깊이-2 왼쪽노드는 ![](https://render.githubusercontent.com/render/math?math=-%5Cfrac%7B49%7D%7B54%7D%5Clog%28%5Cfrac%7B49%7D%7B54%7D%29%20-%20%5Cfrac%7B5%7D%7B54%7D%5Clog%28%5Cfrac%7B5%7D%7B54%7D%29&mode=inline)와 같은 엔트로피 값을 가진다.
###### Equation 6-3. 엔트로피
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/Eq6-3.png)

그러면 우리는 gini 불순도와 엔트로피중 무엇을 사용해야하는가? 사실 대부분의 경우에서 두개가 큰 차이를 보이지는 않는다. 동일한 트리를 사용하기 때문이다. gini 불순도 측정법이 조금 더 연산을 빠르게 해서 기본값으로 설정되어 있는것이다. 하지만 두개가 다르다면 gini 불순도가 트리의 가지중에서 자주 나오는 클래스 값으로 고립되는 경향을 보이긴 한다. 반면에 이런경우 엔트로피가 조금 더 균형된 트리를 만들어주는 경향이 있으므로 때에 맞게 사용하면 된다.
# 정형화 하이퍼파라미터 (Regularization Hyperparameters)
의사결정트리는 학습 데이터에 대하여 매우 적은 가정만을 내린다. (선형 학습 모델들하고는 대조적인 면인데, 이 모델들은 데이터가 선형이라는 가정을 분명하게 내리고 시작한다.) 만약 의사결정트리에 제한이 걸려있지 않다면 트리 구조는 학습 데이터에 그 구조 자체를 받아들이면서 데이터 의존적 학습을 하지만 과잉학습을 하게될 경향이 있다. 이런 모델을 종종 비파라미터모델이라고 하며 어떠한 파라미터도 가지지 않기 때문이아니라(종종 많이 가지기도한다), 학습전에 파라미터의 수가 정해져있지 않기 때문인데, 이는 모델 구조가 데이터에 매우 의존적이라는 것이다.  대조적으로, 선형학습모델같은 파라미터모델은 미리 정해진 파라미터들을 가지고 있는데 자유도가 제한이 걸려있게되고 이는 과잉학습의 위험을 줄여주게된다. (하지만 underfitting의 위험이 증가하게된다.)

학습데이터를 과잉학습하는 것을 피하려면 학습중에 의사결정트리의 자유도를 제한할 필요가 있다. 지금은 아주 잘알고있는 정형화(Regularization)을 사용할 것이다. 정형화 하이퍼파라미터는 사용하는 알고리즘에 의존적이지만 일반적으로 최소한 의사결정트리의 최대 깊이를 제한해줄 수 있다. Scikit-Learn에서 `max_depth`라는 하이퍼파라미터로 통제를 한다(기본값이 `None`으로 되어있는데 이는 제한되어있지 않다는 것을 의미한다). `max_depth`를 줄이는 것은 모델이 정형화되기 때문에 과잉 학습의 위험이 줄어들게 된다.

`DecisionTreeClassifier` 클래스는 의사결정 트리의 형태를 동일하게 제한하는 몇가지 다른 파라미터들을 가지고 있다. 
* `min_samples_split` : 쪼개기 작업을 하기전 한 노드가 가져야만하는 최소 샘플의 수 
* `min_samples_leaf` : 한 리프노드가 가져야만 하는 최소 샘플의 수
* `min_weight_fraction_leaf` : `min_samples_leaf`과 비슷하긴 하지만 가중치가 적용된 인스턴스의 총 수에 대한 분수로 표현이됨
* `max_leaf_nodes` : 리프노드 최대수
* `max_features` : 각 노드에서 나누는 횟수에 대해서 평가되는 특징값의 최대수..?

`min_*`하이퍼파라미터 값을 증가시키는 거나 `max_*`하이퍼파라미터값을 줄이는 것은 모델을 정형화하는 것이다. 
```
다른 알고리즘들은 제한사항 없이 의사결정트리를 먼저 학습시켜보고 필요없는 노드를 제거해나간다. 자식 노드가 
모두 리프 노드들인 노드는 순도 개선이 통계적으로 유효하지 못하다면 불필요하다고 고려될 수 있다. 카이제 
제곱 검정법(χ^2 test)같은 표준 통계 검정법은 평가하고자 하는 측정치에 대해서 그것이 모두를 대변해줄 수
있는지에 대해서 평가하는 것으로 이를 귀무가설(null hypothesis)이라고도 하며 두 집단을 대상으로 
실험을 했을 때 각 집단에서 동일한 결과가 나올 것이라는 가설을 일컫는다. 그래서 귀무가설을 세우면서 필요한
노드와 그렇지 못한 노드들을 구분해내는 것이다.
```
아래의 그림은 chapter 5에서 소개가 된 moons 데이터 세트를 활용하여 의사결정나무를 학습시킨 것이다. 왼쪽의 그림은 기본값 하이퍼 파라미터로 모델을 학습시킨 것이고(예:제한없음), 오른쪽은 `min_samples_leaf`를 4라고 설정해놓고 학습을 시킨 의사결정나무이다. 이는 왼쪽의 그림은 과잉학습되었고, 오른쪽 그림은 적당히 일반화를 잘할 것이라는 것을 보여주고 있다.
###### 그림 6-3. `min_samples_leaf`를 사용한 정형화
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-3.png)
# 회귀
의사결정나무은 회귀를 수행할 수 있는 능력을 가지고 있다. `max_depth`를 2라고 설정하고 노이즈가 있는 2차함수 데이터 세트로 Scikit-Learn의 `DecisionTreeRegressor` 클래스를 사용해서 회귀트리를 구현해보자. 
```
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(X, y)
```
위의 나무에 대한 결과는 아래 그림으로 표현되어 있다.
###### 그림 6-4. 회귀에 대한 의사결정나무
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-4.png)

이 트리는 우리가 앞에서 구현했던 분류 트리와 매우 흡사해보인다. 주된 차이점은 각각의 토드에서 클래스를 예측하는 것이 아닌 값을 예측하는 것이다. 예를들어 우리가 새로운 인스턴스 x1=0.6에 대해서 예측을 내려본다고 하자. 트리의 루트에서 부터 시작해서 결국에는 값을 0.1106이라고 예측하는 리프노드에 도달하게 될 것이다. 이 예측값은 이 리프노드와 연관된 110개의 학습 인스턴스의 평균 타겟 값이다. 이 예측 결과는 전체 110개의 인스턴스에 대하여 MSE를 0.0151이라고 제시한다. 

이 학습 모델의 예측 값은 아래의 왼쪽 그림에 잘 표현되어 있다. `max_depth`를 3이라고 설정한다면 오른쪽에 표현되어 있는 예측값을 얻게될 것이다. 각각의 지역에 대해서 예측된 값들은 항상 그 지역의 인스턴스의 평균 타겟 값이라는 것을 알아두자. 알고리즘은 학습 인스턴스를 예측되는 값에 가능한 가깝게 하는 방법으로 각 지역을 쪼갠다.
###### 그림 6-5. 두개의 의사결정나무 회귀모델의 예측치
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-5.png)

CART 알고리즘은 불순도를 줄이는 방법으로 학습 데이터 세트를 쪼개는 대신에 앞과 같은 방법으로 작동한다. 즉 MSE를 최소화 시키는 방법으로 학습시키는 것이며 아래의 공식이 이를 알고리즘이 최소화하려고 하는 손실함수를 보여준다. 
###### Equation 6-4. 회귀에 대한 CART 손실함수
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/Eq6-4.png)

분류에서 했던 것처럼, 의사결정트리는 회귀에서도 오버피팅하기 쉽다. 어떠한 정형화없이(기본값 하이퍼 파라미터를 사용해서) 학습을 하게된다면 아래의 왼쪽에 나와있는 그림처럼 예측된 값을 얻게될 것이다. 이는 학습데이터에 대해서 아주 나쁘게 과잉학습한 것이다. `min_samples_leaf`를 10으로 설정하면 아래의 오른쪽에 나와있는 그림처럼 좀 더 좋은 모델을 얻을 수 있을 것이다.  
###### 그림 6-6. 의사결정나무 회귀 모델 정형화하기
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-6.png)
# 불안정성(instability)
다행히도 지금은 의사결정트리가 대부분의 경우에서 아주 잘 작동했다. 의사결정트리는 이해하고 설명하기 쉬우며, 사용하기도 간편하고 다재다능하며, 매우 강력했다. 하지만 몇가지 제한사항을 지니고있는데, 첫번째로 알아차렸다시피, 의사결정트리는 수직,직각의 의사 결정선을 아주아주 좋아한다. (모든 분할들이 축에따라 수직,수평으로 쪼개어져왔다) 그래서 이는 학습 데이터를 회전 시키는 경우에는 아주 민감하다. 예시로 아래의 왼쪽 그림은 선형으로 아주 이쁘게 나누어져있지만 반면에 오른쪽 그림은 데이터 세트를 45도 회전시킨 것으로, 의사결정선이 불필요하게 구불구불해보인다. 비록 두개 모두 학습 데이터 세트에 대해서 학습을 잘 하였지만, 오른쪽에 있는 모델은 일반화를 잘 못할 것이다. 이 문제를 제한하는 방법은 PCA를 사용하는 것인데(Chapter 8에서 다룰 것이다), 이는 종종 학습데이터가 올바른 방향을 잡을 수 있게 해준다. 
###### 그림 6-7. 학습데이터 회전에 대한 민감성
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-7.png)

좀 더 일반적으로, 의사결정트리의 주된 이슈는 학습데이터에 조그마한 변화가 있어도 매우 민감하다는 것이다. 예로들어 만약 iris 학습 데이터 세트에서 가장 넓은 Iris-Versicolor를 제거하고(꽃잎 길이:4.8cm, 꽃잎너비: 1.8cm) 새롭게 의사결정나무를 학습시키면, 아래의 그림에서 보여지는 모델을 얻게되는데, 보다시피, 그림 6-2에서 보여지는 이전의 의사결정 트리하고는 매우 다른 것을 알 수 있다. 실제로 Scikit-Learn에서 제공하는 학습 알고리즘은 확률적인 알고리즘들이기 때문에, 같은 학습 데이터에 대해서도 매우 다른 학습 모델 결과를 얻게 될지도 모른다.(`random_stare` 하이퍼파라미터를 설정함에도 불구하고 말이다)
###### 그림 6-8. 학습데이터 세부사항들에 대한 민감성
![](https://github.com/Hahnnz/handson_ml-Kor/blob/master/Book_images/06/6-8.png)

랜덤 포레스트는 많은 트리들에 대해서 예측치를 평균냄으로써 불안정성을 제한하는데, 이는 다음 Chapter에서 다룰 것이다.

**[뒤로 돌아가기](https://github.com/Hahnnz/handson_ml-Kor/)**
