{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터 세트로 정확도 97%가 넘는 MNIST데이터세트 분류 모델을 구현해보자.\n",
    "   * Hint : `KNeighborsClassifiers`가 이 문제에 대해서 잘 작동할 것이다. 우리는 좋은 하이퍼 파라미터 값은 찾기만 하면된다. (`weight`와 `n_neighbors` 하이퍼 파라미터에 대해서 그리드 탐색을 해보자.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=4, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#MNIST 데이터 세트 다운로드\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# 데이터와 레이블 데이터 설정 및 학습데이터 테스트 데이터 분할\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# KNN 모델 정의\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의**: 아래의 학습 코드는 학습시간이 정말 너무 오래 걸리므로 실행전 주의해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] weights=uniform, n_neighbors=3 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=3, score=0.9717617659308622, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=3 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 44.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  weights=uniform, n_neighbors=3, score=0.9706715547408765, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=3 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 89.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  weights=uniform, n_neighbors=3, score=0.9689166666666666, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=3 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=3, score=0.968575477202634, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=3 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=3, score=0.9704068022674225, total= 9.1min\n",
      "[CV] weights=distance, n_neighbors=3 .................................\n",
      "[CV]  weights=distance, n_neighbors=3, score=0.9723448563098709, total= 9.1min\n",
      "[CV] weights=distance, n_neighbors=3 .................................\n",
      "[CV]  weights=distance, n_neighbors=3, score=0.9716713881019831, total= 9.1min\n",
      "[CV] weights=distance, n_neighbors=3 .................................\n",
      "[CV]  weights=distance, n_neighbors=3, score=0.9700833333333333, total= 9.2min\n",
      "[CV] weights=distance, n_neighbors=3 .................................\n",
      "[CV]  weights=distance, n_neighbors=3, score=0.9700758522964075, total= 9.1min\n",
      "[CV] weights=distance, n_neighbors=3 .................................\n",
      "[CV]  weights=distance, n_neighbors=3, score=0.971407135711904, total= 9.1min\n",
      "[CV] weights=uniform, n_neighbors=4 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=4, score=0.9690129112869638, total= 9.2min\n",
      "[CV] weights=uniform, n_neighbors=4 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=4, score=0.9682552907848692, total= 9.1min\n",
      "[CV] weights=uniform, n_neighbors=4 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=4, score=0.9675833333333334, total= 9.2min\n",
      "[CV] weights=uniform, n_neighbors=4 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=4, score=0.9673251646244895, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=4 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=4, score=0.970323441147049, total= 9.1min\n",
      "[CV] weights=distance, n_neighbors=4 .................................\n",
      "[CV]  weights=distance, n_neighbors=4, score=0.9730112453144523, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=4 .................................\n",
      "[CV]  weights=distance, n_neighbors=4, score=0.9722546242292951, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=4 .................................\n",
      "[CV]  weights=distance, n_neighbors=4, score=0.9699166666666666, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=4 .................................\n",
      "[CV]  weights=distance, n_neighbors=4, score=0.9709093940151705, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=4 .................................\n",
      "[CV]  weights=distance, n_neighbors=4, score=0.9719906635545181, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=5 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=5, score=0.9697625989171179, total= 9.0min\n",
      "[CV] weights=uniform, n_neighbors=5 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=5, score=0.9701716380603232, total= 9.1min\n",
      "[CV] weights=uniform, n_neighbors=5 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=5, score=0.9694166666666667, total= 9.1min\n",
      "[CV] weights=uniform, n_neighbors=5 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=5, score=0.9681587063432525, total= 9.1min\n",
      "[CV] weights=uniform, n_neighbors=5 ..................................\n",
      "[CV]  weights=uniform, n_neighbors=5, score=0.9689896632210737, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=5 .................................\n",
      "[CV]  weights=distance, n_neighbors=5, score=0.9703456892961266, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=5 .................................\n",
      "[CV]  weights=distance, n_neighbors=5, score=0.9713381103149475, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=5 .................................\n",
      "[CV]  weights=distance, n_neighbors=5, score=0.9704166666666667, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=5 .................................\n",
      "[CV]  weights=distance, n_neighbors=5, score=0.969409018921397, total= 9.0min\n",
      "[CV] weights=distance, n_neighbors=5 .................................\n",
      "[CV]  weights=distance, n_neighbors=5, score=0.9706568856285428, total= 9.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 1345.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'weights': ['uniform', 'distance'], 'n_neighbors': [3, 4, 5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 4, 'weights': 'distance'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97161666666666668"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97140000000000004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "픽셀단위로 MNIST 데이터 세트를 어느 방향(아래, 위, 왼쪽, 오른쪽)으로든 움직이게 하는 함수를 작성해보자. 그리고 학습 데이터 세트에있는 각각의 이미지에 대해서 네 방향으로 움직인 이미지를 만들어보고 그 이미지들은 학습 데이터 세트에 추가해보자. 마지막으로 이 확장한 학습 데이터 세트를 가지고 우리의 최고의 모델을 학습시켜보고 테스트 데이터 세트로 그 정확도를 측정해보자. 우리가 만든 모델이 심지어 잘 수행되는지 조사해보라. 이러한 인공적으로 학습 데이터를 증가시키는 것을 `Data argumentation` 혹은 `training set expansion`이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(image, dx, dy):\n",
    "    image = image.reshape((28, 28))\n",
    "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
    "    return shifted_image.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAADTCAYAAACx18q9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2adfbea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = X_train[1000]\n",
    "shifted_image_down = shift_image(image, 0, 5)\n",
    "shifted_image_left = shift_image(image, -5, 0)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Original\", fontsize=14)\n",
    "plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.subplot(132)\n",
    "plt.title(\"Shifted down\", fontsize=14)\n",
    "plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.subplot(133)\n",
    "plt.title(\"Shifted left\", fontsize=14)\n",
    "plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented = [image for image in X_train]\n",
    "y_train_augmented = [label for label in y_train]\n",
    "\n",
    "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "    for image, label in zip(X_train, y_train):\n",
    "        X_train_augmented.append(shift_image(image, dx, dy))\n",
    "        y_train_augmented.append(label)\n",
    "\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "y_train_augmented = np.array(y_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "X_train_augmented = X_train_augmented[shuffle_idx]\n",
    "y_train_augmented = y_train_augmented[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.fit(X_train_augmented, y_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97629999999999995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 뿔려주었더니 정확도가 약 0.5%정도 상승했다. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타이타닉(*titanic*) 데이터 세트로 한번 도전해보자. [Kaggle](https://www.kaggle.com/c/titanic)에서 처음 시작해보는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표는 승객의 나이, 성별, 승객이 승선한 객실클래스 등등 같은 속성값들에 기반해서 승객이 살아남을 수 있는지 없는지를 예측하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 불러오자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_titanic_data(\"train.csv\")\n",
    "test_data = load_titanic_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 이미 학습 데이터랑 훈련 데이터로 나뉘어져 있다. 하지만, 테스트 데이터는 레이블을 포함하고 있지않다. 우리의 목표는 학습데이터를 사용해서 최고의 모델을 만든뒤에, 테스트 데이터에 대해서 예측값을 얻는 것이다. 그 결과를 Kaggle에 올려서 최종 점수를 보는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 세트에서 상위 몇가지 행들을 한번 둘러보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "속성값들은 다음과 같은 뜻을 가지고 있다.\n",
    "\n",
    "* **Survived**: 이것이 타겟 값으로, 0은 승객이 살아남지 못했다는 것을 의미하고, 반면에 1은 승객이 생존했다는 것을 의미한다.\n",
    "* **Pclass**: 객실클래스.\n",
    "* **Name**, **Sex**, **Age**: 설명이 필요 없다.\n",
    "* **SibSp**: 타이타닉에 탑승한 승객중 얼마나 많은 형제자매나 배우자가 탑승하였는지를 말한다.\n",
    "* **Parch**: 타이타닉에 탑승한 승객중 얼마나 많은 아이들과 부모가 있엇는지를 나타낸다.\n",
    "* **Ticket**: 티켓 번호\n",
    "* **Fare**: 지불한 돈 (단위 : 파운드)\n",
    "* **Cabin**: 승객의 객실 번호\n",
    "* **Embarked**: 타이타닉에 승성한 승객의 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 얼마나 손실되는지 좀 더 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**, **Cabin**와 **Embarked** 속성값중에서 어떤 것들은 `null`(891개 이하정도가 값을 가지고 있음)이며, 특히 **Cabin**은 77%정도가 `null`이다. 지금은 **Cabin** 속성값은 무시하고 다른 속성값들에 대해서 집중해보자. **Age** 속성값은 19%정도가 `null`값이라 이 속성들을 어떻게 해야할지 정해야한다. null을 평균 나이로 대체하는 것이 어느정도 괜찮아보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**과 **Ticket** 속성값은 값을 가지고는 있지만 학습 모델이 받아들일 수 있도록 하는 유용한 숫자값들로 바꿔줄 수 있을 것이다. 하지만 지금은 무시를 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자로 된 속성값들은 무엇이 있는지 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 헉, 오직 38%정도만이 생존(**Survived**)했다. :(  그래도 40%정도에 가까워, 정확도는 우리 모델을 평가하는데 합리적인 측정법이 될 것이다.\n",
    "* **Fare**의 평균은 £32.20(파운드)이며, 이는 생각보다 비싸지 않다. (그래도 과거의 돈은 지금보다 더 비싼 값어치를 가지고 있었을 것이다.)\n",
    "* **Age** 평균은 30세 이전 정도이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 타겟값들이 0혹은 1인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 카테고리로 된 속성값들을 빠르게 훑어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    491\n",
       "1    216\n",
       "2    184\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Pclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      577\n",
       "female    314\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embarked** 속성값은 승객이 어디에 승선하였는지를 나타내준다. `C=Cherbourg`, `Q=Queenstown`, `S=Southampton`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CategoricalEncoder`클래스는 우리가 카테고리로 된 값들을 one-hot 벡터로 변환하게 해줄 것이다. one-hot encoding은 곧 Scikit-learn에 추가될 것이다. 그동안에는 아래의 코드를 사용하자.(Pull Request #9151에 있는 코드를 복사한 것임)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 전처리 파이프 라인을 만들어보자. 이전 Chapter에서 사용했던 `DataFrame`에서 특정 속성값들을 골라오기 위해 `DataframeSelector`를 다시 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자로 된 속성값들에 대한 파이프라인도 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n",
    "        (\"imputer\", Imputer(strategy=\"median\")),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.    ,   1.    ,   0.    ,   7.25  ],\n",
       "       [ 38.    ,   1.    ,   0.    ,  71.2833],\n",
       "       [ 26.    ,   0.    ,   0.    ,   7.925 ],\n",
       "       ..., \n",
       "       [ 28.    ,   1.    ,   2.    ,  23.45  ],\n",
       "       [ 26.    ,   0.    ,   0.    ,  30.    ],\n",
       "       [ 32.    ,   0.    ,   0.    ,   7.75  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string 카테고리 열을 전가할(imputer) 필요가 잇다. (평범한 `Imputer`는 이 기능을 제공하지 않는다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stackoverflow.com/questions/25239958 에 영감을 받아 구현한 코드\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                       index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 카테고리 속성값도 처리해주는 파이프라인도 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, 숫자로된 파이프라인과 카테고리로 된 파이프라인을 합쳐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "멋지다! 이제 원래의 데이터를 가져와서 우리가 원하는 그 어떤 기계학습 알고리즘에 입력할 수 있는 숫자로 구성된 입력값을 출력해주는 훌륭한 전처리 파이프라인을 구현했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.,   1.,   0., ...,   0.,   0.,   1.],\n",
       "       [ 38.,   1.,   0., ...,   1.,   0.,   0.],\n",
       "       [ 26.,   0.,   0., ...,   0.,   0.,   1.],\n",
       "       ..., \n",
       "       [ 28.,   1.,   2., ...,   0.,   0.,   1.],\n",
       "       [ 26.,   0.,   0., ...,   1.,   0.,   0.],\n",
       "       [ 32.,   0.,   0., ...,   0.,   1.,   0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블 값을 가져오는 것도 잊지말자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 분류모델을 학습시킬 준비가 되었다. `SVC`를 활용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훌륭하다, 모델이 학습되었다. 이제 테스트 데이터 세트에 대해서 예측값을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_pipeline.transform(test_data)\n",
    "y_pred = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 이제 우리는 이런 예측 결과를 CSV파일로 만들고(Kaggle에 제출해야 하는 형식임), 이를 Kaggle에 올려본 뒤에 최고의 결과치를 가질 수 있도록 기도하면된다. 하지만 우리가 기도하는 것보다 더 좋은걸 해볼 수 있다. 우리모델이 얼마나 좋은지 평가하는데 교차검증법을 사용하려는 생각을 하지 못했을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73652508228350921"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋아, 73%이상의 정확도를 가지며 분명이 막 찍는 것보다는 좋지만, 좋은 결과값은 아니다. Kaggle에 있는 타이타닉 데이터 공모전에 대한 [leaderboard](https://www.kaggle.com/c/titanic/leaderboard)를 보면, 상위 10%에 있는 캐글러(Kaggler)의 것은 80%이상 올라간다. 일부는 100%를 찍으며, [list of victims](https://www.encyclopedia-titanica.org/titanic-victims/)을 찾아보면 아보다 조금 기계학습이 그들의 수행도가 더\n",
    "\n",
    "그래서 이제 정확도가 80%에 가까워지도록 모델을 만들어보자.\n",
    "\n",
    "Okay, over 73% accuracy, clearly better than random chance, but it's not a great score. Looking at the [leaderboard](https://www.kaggle.com/c/titanic/leaderboard) for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the [list of victims](https://www.encyclopedia-titanica.org/titanic-victims/) of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let's try to build a model that reaches 80% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomForestClassifier`로 시도해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8115690614005221"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확실히 더 좋다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 좀 더 향상시키기 위해서는\n",
    "* 다른 모델과도 비교해보고 교차 검증법과 그리드 서치를 사용하여 하이퍼 파라미터를 조율해보자.\n",
    "* 좀 더 특징값들에 대해서 다루어보자. 예를들면,\n",
    "  * **SibSp**와 **Parch**를 그들의 합으로 바꾸어보자\n",
    "  * **Survived** 속성값들과 잘 연관된 것들이 이름에 있는지 확인해보자 (이름에 \"Countess\"(여자백작)가 포함되어 있다면 좀 더 생존할 가능성이 있다.)\n",
    "* 숫자 속성값들을 카테고리 속성값으로 변환해보자. 예를들어, 나이 세대마다 생존 확률이 매우 다르다. (아래를 참고하라) 그래서 **age bucket**이라는 카테고리를 만들고 **age**속성 대신에 사용해보자. 비슷한 방법으로 혼자 여행한 사람들에 대한 특별한 카테고리를 만들어보는 것도 괜찮은 방법일 수도있다. (아래를 참고하자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgeBucket</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>0.362745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.423256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45.0</th>\n",
       "      <td>0.404494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60.0</th>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75.0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Survived\n",
       "AgeBucket          \n",
       "0.0        0.576923\n",
       "15.0       0.362745\n",
       "30.0       0.423256\n",
       "45.0       0.404494\n",
       "60.0       0.240000\n",
       "75.0       1.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
    "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RelativesOnboard</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.552795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.578431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Survived\n",
       "RelativesOnboard          \n",
       "0                 0.303538\n",
       "1                 0.552795\n",
       "2                 0.578431\n",
       "3                 0.724138\n",
       "4                 0.200000\n",
       "5                 0.136364\n",
       "6                 0.333333\n",
       "7                 0.000000\n",
       "10                0.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
    "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 스팸 분류 모델을 구현해보자 **(도전과제)**\n",
    "   * [Apache SpamAssassin's public datasets](https://spamassassin.apache.org/old/publiccorpus/)에서 스팸과 스팸이 아닌 메일에 대한 예시를 다운로드 하자.\n",
    "   * 데이터셋의 압축을 풀고 데이터 형식에 대해서 익숙해지자\n",
    "   * 특징 벡터로 각각의 이메일을 변환하는 데이터 사전준비 파이프라인을 작성해보자. 전처리 파이프라인은 이메일을 각각의 가능한 단어들의 부재와 존재를 나타내주는 (희소) 벡터로 변환해야한다. 예로, 만약 모든 메일이 네 가지 단어(\"Hello\", \"how\", \"are\", \"you\")를 가지고 있다면, \"Hello you Hello Hello you\"라는 이메일은 [1,0,0,1]이라는 벡터로 변환되어야한다.(벡터의 의미는 hello와 you가 존재하고, how와 are 이 없기 때문에 1,0,0,1로 표현된 것이다.) 아니면 나온 단어 수를 세어서 벡터 값으로 활용할 수 있을 것이다. 예로 위의 경우를 표현한다면 [3,0,0,2]로 표현할 수 있을 것이다.\n",
    "   * 다른 분류 모델을 시도해보고, recall과 precision이 높은 좋은 스팸 메일 분류 모델을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 데이터를 불러와보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 모든 이메일들을 불러와보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이메일들을 파싱(Parsing)하기 위해 Python의 `email`모듈을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 어떻게 생겼는지 감을 얻기 위해 스팸이 아닌 메일과 스팸인 메일을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇몇 이메일들은 이미지나 첨부가 달려있는 메일도 있다. 우리가 가진 구조의 다양한 종류들을 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스팸이 아닌 메일이 좀 더 종종 평서문으로 되어있는 반면에 스팸메일은 HTML이 꽤 많이 실려있다. 더 나아가 스팸이 아닌 메일은 PGP를 사용하지만, 반면에 스팸메일은 그렇지 못하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이메일의 헤더(header)를 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header,\":\",value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보내는 사람의 이메일 주소같은 유용한 정보가 많을 것이다. 하지만 우리는 `Subject`라는 헤더에 집중해볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터에 대해서 더 살펴보기전에 학습 데이터와 테스트 데이터로 나누는 것을 잊지말자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋다, 전처리 함수를 작성해보자. 먼저 HTML을 평서문으로 변환해주는 함수가 필요할 것이다. 아마도 최고의 방법은 [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)라이브러리를 사용하는 것이지만, 이 프로젝트의 의존성을 더이상 추가하고 싶지 않으므로, 빠르지만 더러운 솔루션을 사용할 것이다. 다음의 함수는 먼저 `<head>` 섹션을 삭제해주고 모든 `<a>`태그를 하이퍼링크로 변환헤주고, 일반 평서문들을 제외한 모든 HTML 태그를 삭제한다. 읽기 좋도록, 한줄에 여러개의 명령어를 추가하고, 마지막엔 (`&gt;`나 `&nbsp;` 같은) HTML 내용을 디코딩(unescape)해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작동하는지 보자. HTML 스팸 메일이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
      "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
      "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
      "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
      "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
      "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
      "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 이것은 결과로 나온 평서문 형식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "REASONS TO INVEST IN CBYI\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋다! 이제 이메일의 형식이 무엇이든 입력으로 받아 평서문으로 반환해주는 함수를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Wat ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dlwp wnfrlfmf cuqhwk\n",
    "Let's throw in some stemming! For this to work, you need to install the Natural Language Toolkit ([NLTK](http://www.nltk.org/)). It's as simple as running the following command (don't forget to activate your virtualenv first; if you don't have one, you will likely need administrator rights, or use the `--user` option):\n",
    "\n",
    "`$ pip install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: stemming requires the NLTK module.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL로 이루어진 곳은 \"URL\"이라는 단어로 바꾸어줄 필요가 있다. [urlextract](https://github.com/lipoja/URLExtract) 라이브러리를 사용해볼 것이다. (가상환경을 켜놓는 것을 잊지말자) 아래의 명령어로 해당 모듈을 설치할 수 있다.\n",
    "\n",
    "`$ pip install urlextract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: replacing URLs requires the urlextract module.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이메일들을 단어의 개수를 세주는 곳에 넣어주기 위한 변환함수에 모두 함깨 넣어줄 수 있게 되었다. 파이썬의 `split()` 함수를 사용해 문장을 단어로 나누어야하며 이는 단어 경계를 스페이스로 사용한다. 이는 수많은 언어들에게 잘 작동하지만 모든 언어에 부합하는 것은 아니다. 중국어나 일본어 스크립트는 일반적으로 단어 사이제 띄어쓰기를 하지 않으며, 베트남어는 종종 음절 사이에 띄어쓰기를 하기도 한다. 이번 예시에서는 괜찮다. 영어로 이루어져 있으니까."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇가지 이메일에 대해서 이 변환함수를 실험해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ Counter({'yawn': 1, 'wrote': 1, 'murcko': 1, 'chuck': 1, 'stuff': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'by': 3, 'to': 3, 'all': 3, 'christianity': 3, 'one': 2, 'has': 2, 'have': 2, 'half': 2, 'jefferson': 2, 'jesus': 2, 'been': 2, 'i': 2, 'on': 2, 'burnt': 1, 'worbois': 1, 'a': 1, 'that': 1, 'are': 1, 'teachings': 1, 'pfp': 1, 'our': 1, 'rogueries': 1, 'innocent': 1, 'remsburg': 1, 'known': 1, 'paul': 1, 'effect': 1, 'www': 1, 'fools': 1, 'not': 1, 'six': 1, 'what': 1, 'perpetrated': 1, 'thomas': 1, 'children': 1, 'man': 1, 'tortured': 1, 'fables': 1, 'earth': 1, 'http': 1, 'upon': 1, 'in': 1, 'william': 1, 'redeeming': 1, 'do': 1, 'examined': 1, 'introduction': 1, 'over': 1, 'word': 1, 'perverted': 1, 'corrupter': 1, 'ever': 1, 'women': 1, 'americans': 1, 'roguery': 1, 'shone': 1, 'short': 1, 'interesting': 1, 'dupes': 1, 'coercion': 1, 'feature': 1, 'world': 1, 'imprisoned': 1, 'e': 1, 'html': 1, 'system': 1, 'some': 1, 'quotes': 1, 'millions': 1, 'importers': 1, 'letter': 1, 'error': 1, 'they': 1, 'were': 1, 'most': 1, 'mythology': 1, 'large': 1, 'postfun': 1, 'founded': 1, 'teaching': 1, 'hypocrites': 1, 'become': 1, 'absurdities': 1, 'superstition': 1, 'superstitions': 1, 'untruths': 1, 'band': 1, 'support': 1, 'com': 1, 'men': 1, 'great': 1, 'again': 1, 'historic': 1, 'first': 1, 'fined': 1, 'led': 1, 'this': 1, 'find': 1, 'particular': 1, 'alike': 1, 'make': 1, 'since': 1, 'other': 1, 'john': 1}),\n",
       "       Counter({'NUMBER': 5, 'http': 4, 'yahoo': 4, 'to': 3, 's': 3, 'com': 3, 'unsubscribe': 2, 'is': 2, 'in': 2, 'martin': 2, 'an': 2, 'forteana': 2, 'we': 2, 'and': 2, 'groups': 2, 'memri': 2, 'uk': 1, 'id': 1, 'articles': 1, 'adamson': 1, 'that': 1, 'based': 1, 'murdered': 1, 'nxieaa': 1, 'non': 1, 'outright': 1, 'rob': 1, 'send': 1, 'www': 1, 'free': 1, 'bin': 1, 'us': 1, 'of': 1, 'yemen': 1, 'don': 1, 'ia': 1, 'page': 1, 'co': 1, 'from': 1, 'all': 1, 'p': 1, 'info': 1, 'unbiased': 1, 'dvds': 1, 'wrote': 1, 'NUMBERgsolb': 1, 'rather': 1, 'ptNUMBERybb': 1, 'docs': 1, 'alternative': 1, 'terms': 1, 'more': 1, 'factually': 1, 'email': 1, 'rundown': 1, 'hamza': 1, 'career': 1, 't': 1, 'should': 1, 'his': 1, 'guardian': 1, 'html': 1, 'on': 1, 'belief': 1, 'your': 1, 'how': 1, 'click': 1, 'including': 1, 'elsewhere': 1, 'muslims': 1, 'group': 1, 'sponsor': 1, 'archives': 1, 'egroups': 1, 'org': 1, 'y': 1, 'join': 1, 'this': 1, 'use': 1, 'iaNUMBER': 1, 'cgi': 1, 'journalist': 1, 'subject': 1, 'for': 1, 'be': 1, 'now': 1, 'mvfiaa': 1, 'tm': 1, 'story': 1, 'area': 1, 'know': 1})], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "올바르게 된 것 같다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 단어의 수를 셀 것이다. 그리고 이를 벡터로 변환해줄 필요가 있다. 이를 위해 `fit()` 함수가 어휘를 구현하고 `transform()` 함수가 어휘를 사용하여 단어 수를 벡터로 변환하도록하는 또 다른 변환함수를 구축 할 것이다. 결과는 희소행렬로 출력이 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [110,   9,  11,   8,   3,   0,   1,   3,   1,   0,   3],\n",
       "       [ 94,   1,   0,   2,   3,   5,   4,   1,   3,   4,   0]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUMBER': 5,\n",
       " 'all': 7,\n",
       " 'and': 3,\n",
       " 'by': 10,\n",
       " 'com': 8,\n",
       " 'http': 6,\n",
       " 'of': 1,\n",
       " 'the': 2,\n",
       " 'to': 4,\n",
       " 'yahoo': 9}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 스팸메일 분류모델을 학습시킬 준비가 되었다 전체 데이터를 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ..................................... , score=0.98, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................... , score=0.9825, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................. , score=0.99125, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98458333333333325"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.4%정도가 넘는다. 처음치고는 나쁘지않다! :) 하지만 우리는 쉬운 데이터 세트를 사용했다는 것을 기억하자. 좀 더 어려운 데이터로 시도해볼 수 있지만 결과가 그렇게 놀랄 정도가 되지는 못할 것이다. 좀 더 다양한 모델들을 시도하고, 최고의 모델을 골라 교차 검증법을 통해 하이퍼 파라미터를 조율하고...\n",
    "\n",
    "그림이 어느정도 나오게되면 일단 잠시 멈추고, 테스트 데이터 세트에 대해서 precision/recall를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00%\n",
      "Recall: 0.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
